{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper : Deep Reinforcement Learning with Double Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges the Q-learning algorithms on grounds of overestimation of action values under certain conditions and provides reasons/solutions for the same. Establishes a *Double Q-learning Algorihtm* (earlier used for tabular settings) and extrapolates it to large-sclae function approximations.\n",
    "\n",
    "Motivation : Standard Q-learning and DQN uses the same values both to select and to evaluate an action.This makes it more likely to select over estimated values , resulting in overoptimistic value estimates.To prevent this , decoupling of the selection from the evaluation can be done.\n",
    "\n",
    "Goal : Extend the work done by van hasselt et al., named Double Q-learning for the tabular setting to work with arbitrary function approximators , including deep NN, naming it as Double DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Idenified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-learning sometimes learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values. These can lead to insufficient function approximations and unwanted noise.\n",
    "- Though being optimistic about action values is a well-known exploration technique( Kaelbling et al., 1996) , being over optimistuc i.e when the action values are over estimated non-uniformaly and not concentrated at the states about which we want to learn, they might negatively effect the quality of the resulting policy ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Q-learning/DQN update, the target value ${\\mathcal{Y}_t}^{Q}$\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathcal{Y}_t^{Q} = \\mathcal{R}_{t+1} + \\lambda \\max \\mathcal{Q}(S_{t+1},a;\\theta_t^{-})  \n",
    "\\end{equation}\n",
    "\n",
    "here, $\\theta_t^{-}$ indicates the use of target networks and is the target network paramters.\n",
    "\n",
    "For the Double DQN update, the target value now becomes \n",
    "\\begin{equation}\n",
    "          \\mathcal{Y}_t^{Q} = \\mathcal{R}_{t+1} + \\lambda \\mathcal{Q}(S_{t+1}, \\argmax  \\mathcal{Q}(S_{t+1},a;\\theta_t);\\theta_t^{-})  \n",
    "\\end{equation}       \n",
    "\n",
    "In DDQN , two value functions are learned assigning each experince randomly to update one of the two value functions. Two sets of weights , $\\theta_t$ (online weights) and $\\theta_t^{-}$ (evaluats the value of the policy), are used one for determining the greedy policy and the other to determine its value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demonstartes that Q-learning can be over optimistic in large-scale problems evne if these are deterministic, due to inherent estimation errors of learning.\n",
    "- Analysis of value estimates on Atari 2600 games demonstrates that these overestimations are more common and severe in pratice that acknowledged.\n",
    "- Double Q-learning cna be used to scale to successfully reduce this overoptimisim, resulting into more stable learning\n",
    "- DDQN , that uses the existing presets of DQN wihtout requiring additonal network paramters and gives better polices and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architeture details\n",
    "\n",
    "The architecture and optimizers used is exactly the one proposed by Mnih et al. in Nature DQN paper (Check DQN notebook)\n",
    "\n",
    "### Hyper parameters\n",
    "\n",
    "Discount $\\lambda$ = 0.99.\n",
    "\n",
    "Learning rate $\\alpha$ = 0.00025.\n",
    "\n",
    "Number of steps b/w target network updates was $\\tau$ = 10,000.\n",
    "\n",
    "50M training steps , evlauted every 1M steps , bets policy was kept as the output of the learning process.\n",
    "Repaly memory size = 1M\n",
    "\n",
    "The memory gets sampled to update the network every 4 steps with minibatches of size 32. \n",
    "The simple exploration policy used is an $\\epsilon$-greedy policy with the $\\epsilon$ decreasing linearly from 1 to\n",
    "0.1 over 1M step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
