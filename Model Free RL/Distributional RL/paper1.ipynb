{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper : A Distributional Perspective on Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper demonstrates the ways in which *value distribution* impacts learning in RL settings. Here, value distribution is the distribution of the random return received by the agent instead of the normal expectation of these returns used earlier. \n",
    "\n",
    "Goal : Introduce a distributional perspective of random rewards $\\mathcal{Z}$ instead of expected reward $\\mathcal{E}$, and changes the normal bellman Equation from .\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathcal{Q}(x,a) = \\mathcal{E} R(x,a) + \\gamma \\mathcal{E} Q(X' ,A')\n",
    "\\end{equation}\n",
    "\n",
    "to \n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathcal{Z}(x,a) = \\mathcal{R}(x,a) + \\gamma \\mathcal{Z} Q(X' ,A')\n",
    "\\end{equation}\n",
    "\n",
    "Thi sdistributional Bellman equation states that the distribution of Z is characterized by the interaction of three random\n",
    "variables: the reward $mathcal{R}$, the next state-action $(X' , A')$  its random return $\\mathcal{Z}(X',A')$. Also , call it the vaie "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
