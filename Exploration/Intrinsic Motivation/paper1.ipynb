{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper : VIME: Variational Information Maximizing Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristic exploration strategies need to used where action-state space discritization is infeasible (for discrete scenarios, exploration is managed by Bayesian RL). The paper introduces a curiosity driven exploration strategy making use of information gain about the agentâ€™s internal belief of the dynamics model as a driving force. Here, agents are encouraged to take actions that result in states they deem surprising - states that caused large updates to the dynamics model distribution.\n",
    "\n",
    "Goal : Introduce a exploration strategy based on maximization of information gain about the agent;s belief of env dynamics (continous state and action spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While their are methods for optimality in discrete action and state space, these methods cannot be applied to high-dimensional deep RL scenarios\n",
    "- Most RL methods rely on simple $\\epsilon$-greedy exploration or adding gaussian noise, there is less work on scalable and effective exploration\n",
    "- Methods like tile coding , coarse discretization does not help for high-dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ethod builds on the theory of curiosity-driven exploration, in which the agent engages in systematic exploration by seeking out state-action regions that are relatively unexplored\n",
    "- The agent is encouraged to take actions that lead to states that are maximally informative about the dynamics model.\n",
    "- The $KL Divergence$ from the agent's new beliefs to the old one , can be interpreted as information gain.\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
